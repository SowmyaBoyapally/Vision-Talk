# Vision-Talk
This work addresses the challenge of interacting with visual content through natural and accessible user interfaces, where most existing systems rely on cloud services or predefined query formats. We present Vision Talk, an offline multimodal image question-answering system that enables users to query images using either text or voice input and receive responses in both textual and spoken form. The system combines image captioning for visual understanding, automatic speech recognition for voice input, and a locally deployed language model for context-aware response generation. Text-to-speech synthesis is used to deliver audible output, enabling hands-free and accessible interaction.
Unlike cloud-centric vision–language systems, the proposed framework operates entirely on local hardware without requiring paid APIs, internet connectivity, or GPU acceleration. The system supports open-ended image-based queries and flexible interaction rather than fixed commands. Experimental observations indicate that the framework produces coherent responses with practical latency on CPU-based systems. The proposed approach demonstrates the feasibility of deploying multimodal image understanding systems in offline and resource-constrained environments, with potential applications in accessibility tools, education, and assistive technologies.

# Keywords
Image Question Answering, Multimodal AI, Vision–Language Interaction, Speech Processing, Offline Deployment
